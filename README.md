This is our Comp545 Final Project
The code is meant to reproduce the result of "The Power of Scale for Parameter-Efficient Prompt Tuning" author Brian Lesterâˆ— Rami Al-Rfou Noah Constant Google Research 
However we want to use Alberta model and Roberta model as the base instead of GPT2.

To run the code:
1. Download the SuperGLUE and LittleGLUE datasets: Place them in the appropriate subfolders. We've included the download links for the full datasets in our report.
2. Run the Jupyter notebook: Simply execute the notebook cells in order, starting from the top. Each dataset should take approximately 4 hours to complete.
3. By default, the code runs with the BoolQ dataset and the Roberta model. However, there are detailed comments within the code itself that explain how to change the model and dataset. Following these comments, you should find it fairly straightforward to execute the code.
